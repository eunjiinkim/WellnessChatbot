#!/usr/bin/env python
# coding: utf-8


import streamlit as st
from transformers import GPT2LMHeadModel, PreTrainedTokenizerFast
import torch

model = GPT2LMHeadModel.from_pretrained('eunjin/kogpt2-finetuned-wellness')

U_TKN = '<usr>'
S_TKN = '<sys>'
EOS = '</s>'
MASK = '<unused0>'
PAD = '<pad>'
SENT = '<unused1>'

tokenizer = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2",
            eos_token=EOS, unk_token='<unk>',
            pad_token=PAD, mask_token=MASK) 



@st.cache(allow_output_mutation=True,suppress_st_warning=True)
def chat(utterance,maxlen,numbeams,sampling,topk,ngramsize,temp):
    with torch.no_grad():
            qs=[]
            q = utterance
            qs.append(q) # history ì €ì¥

            a=''
            user = U_TKN + q + SENT + a
            encoded = tokenizer.encode(user)
            input_ids = torch.LongTensor(encoded).unsqueeze(dim=0)
            output = model.generate(input_ids,max_length=maxlen,
                                         num_beams=numbeams, do_sample=sampling, 
                                         top_k=topk, no_repeat_ngram_size=ngramsize,
                                        temperature=temp)
            a=tokenizer.decode(output[0])
            idx = torch.where(output[0]==tokenizer.encode('<sys>')[0])
            chatbot = tokenizer.decode(output[0][int(idx[0])+1:], skip_special_tokens=True)
 
            if 'ë‹µë³€' in a: # ì‘, ì•„ë‹ˆ ë“±ì´ inputìœ¼ë¡œ ë“¤ì–´ì™”ì„ ë•Œ
                a_new = ''
                user = U_TKN + ''.join(qs[-2:]) + SENT + a_new # ì§ì „ history ê°€ì§€ê³  ì™€ì„œ sentiment ê³ ë ¤í•´ì£¼ê¸°
                encoded = tokenizer.encode(user)
                input_ids = torch.LongTensor(encoded).unsqueeze(dim=0)
                output = model.generate(input_ids,max_length=maxlen,
                                         num_beams=numbeams, do_sample=sampling, 
                                         top_k=topk, no_repeat_ngram_size=ngramsize,
                                        temperature=temp)
                a_new = tokenizer.decode(output[0], skip_special_tokens=True)
                idx = torch.where(output[0]==tokenizer.encode('<sys>')[0])
                chatbot = tokenizer.decode(output[0][int(idx[0])+1:], skip_special_tokens=True)
                
                result = chatbot.strip()
            
            else: 
                result = chatbot.strip()
                
            

    return result


def main():

    # Title
    st.title("í•œêµ­ì–´ ì •ì‹ ê±´ê°• ìƒë‹´ ì±—ë´‡")
    st.subheader("")
    st.markdown("""
        ğŸ™‚ wellness ì±—ë´‡ ë°ì´í„°(AI hub)ì™€ ì¼ìƒëŒ€í™” ì±—ë´‡ ë°ì´í„°(github,@songys)ì„ kogpt2(skt)ì— fine-tuningí•œ ì±—ë´‡ì…ë‹ˆë‹¤.
        """
        """
        ğŸ’œ ê° ì±—ë´‡ ë°ì´í„°ì˜ ê°ì •sentiment ì •ë³´ë¥¼ í•¨ê»˜ ìƒì„±í•˜ë„ë¡ í•™ìŠµí•˜ì—¬ ì‚¬ìš©ìì—ê²Œ ìœ„ë¡œì™€ ê³µê°ì„ ì£¼ë„ë¡ í•˜ì˜€ìŠµë‹ˆë‹¤.
        """)

    st.subheader("ì±—ë´‡ê³¼ ëŒ€í™”í•´ë³´ì„¸ìš”!")

    utterance = st.text_input("ë‚˜: ", "ì˜¤ëŠ˜ì€ ì–´ë–¤ ê¸°ë¶„ì´ì‹ ê°€ìš”?")
    
    st.sidebar.subheader("Generation Settings")
    maxlen=st.sidebar.slider("max length of the sequence", 30, 60,value=50)
    numbeams=st.sidebar.slider("number of beams", 5,20, value=10)
    topk=st.sidebar.slider("top k sampling", 10, 50, value=20)
    ngramsize=st.sidebar.slider("Ngram size not to repeat", 1,4,value=2)
    temp=st.sidebar.slider("temperature: threshold probability of each token", 0.7, 0.95, value=0.85)
    sampling=st.sidebar.checkbox("do sampling", value=False)
    
    
    st.sidebar.subheader("About App")
    st.sidebar.text("Streamlit")
    
    if st.button("ì „ì†¡"):
        result = chat(utterance,maxlen,numbeams,sampling,topk,ngramsize,temp)
        st.text_area("ì±—ë´‡: ", value=result)

    st.sidebar.subheader("ê°œë°œì")
    st.sidebar.text("ê¹€ì€ì§„, jyej3154@snu.ac.kr")
    st.sidebar.text("ì„œìš¸ëŒ€í•™êµ ì–¸ì–´í•™ê³¼ ì„ì‚¬ê³¼ì •")
    
    st.subheader("ì°¸ì¡°")
    st.markdown("kogpt2 ê¸°ë°˜ ì±—ë´‡: https://github.com/haven-jeon/KoGPT2-chatbot")
    st.markdown("pytorch lightning: https://www.pytorchlightning.ai/")
    st.markdown("huggingface transformers: https://huggingface.co/")
    st.markdown("wellness dataset: https://aihub.or.kr/keti_data_board/language_intelligence")
    
if __name__ == '__main__':
	main()

